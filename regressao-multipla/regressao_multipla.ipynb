{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesta tarefa será implementada o algoritmo de regressão linear com múltiplas variáveis, utilizando a versão vetorizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Os dados treinados serão de notas de alunos da UFCG, onde queremos prever o CRA, baseado em algumas disciplinas do primeiro período."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mse_vectorized(w, H, Y):\n",
    "    res = Y - np.dot(H, w)\n",
    "    totalError = np.dot(res.T, res)\n",
    "    return (totalError / float(len(Y)))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient_vectorized(w_current, H, Y, learningRate):\n",
    "    # y - Hw\n",
    "    res = Y - np.dot(H, w_current)\n",
    "    \n",
    "    # −2HT (y−Hw)\n",
    "    gradiente = np.multiply(-2, np.dot(H.T, res))\n",
    "    \n",
    "    # alpha * (−2HT (y−Hw))\n",
    "    gradiente_atenuado = np.multiply(learningRate, gradiente)\n",
    "    \n",
    "    # w(t+1) = w(t) - alpha * (−2HT (y−Hw))\n",
    "    new_w = np.subtract(w_current, gradiente_atenuado)\n",
    "    \n",
    "    return [new_w, gradiente]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_runner_vectorized(starting_w, X,Y, learning_rate, epsilon):\n",
    "    w = starting_w\n",
    "    grad = np.array([np.inf] * len(X[0]))\n",
    "    i = 0\n",
    "    while (np.linalg.norm(grad) >= epsilon):\n",
    "        w, grad = step_gradient_vectorized(w, X, Y, learning_rate)\n",
    "        \n",
    "        if i % 50000 == 0:\n",
    "            print(\"MSE na iteração {0} é de {1}\".format(i,compute_mse_vectorized(w, X, Y)))\n",
    "        i+= 1\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with error = 54.4799538556\n",
      "Running...\n",
      "MSE na iteração 0 é de 13.5401136888\n",
      "MSE na iteração 50000 é de 0.417316489568\n",
      "MSE na iteração 100000 é de 0.413093359157\n",
      "MSE na iteração 150000 é de 0.411853190403\n",
      "MSE na iteração 200000 é de 0.411489001191\n",
      "MSE na iteração 250000 é de 0.411382053018\n",
      "MSE na iteração 300000 é de 0.411350646509\n",
      "Gradiente descendente convergiu com w0 = [1.69701142], w1 = [0.10377072], w2 = [0.04829928], w3 = [0.16400561], w4 = [0.38324514], w5 = [0.02077901], error = 0.411349181254\n",
      "Versão vetorizada rodou em: 7909.50584412 ms\n"
     ]
    }
   ],
   "source": [
    "points = np.genfromtxt(\"data/sample_treino.csv\", delimiter=\",\")\n",
    "points = np.c_[np.ones(len(points)),points][1::]\n",
    "X = points[:, :-1]\n",
    "Y = points[:, -1:]\n",
    "init_w = np.zeros((len(points[0]) - 1,1))\n",
    "learning_rate = 0.00001\n",
    "#num_iterations = 10000\n",
    "epsilon = 0.05\n",
    "print(\"Starting gradient descent with error = {0}\".format(compute_mse_vectorized(init_w, X,Y)))\n",
    "print(\"Running...\")\n",
    "tic = time.time()\n",
    "w = gradient_descent_runner_vectorized(init_w, X,Y, learning_rate, epsilon)\n",
    "toc = time.time()\n",
    "print(\"Gradiente descendente convergiu com w0 = {0}, w1 = {1}, w2 = {2}, w3 = {3}, w4 = {4}, w5 = {5}, error = {6}\".format(w[0], w[1], w[2], w[3], w[4], w[5], compute_mse_vectorized(w,X,Y)))\n",
    "print(\"Versão vetorizada rodou em: \" + str(1000*(toc-tic)) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pode ser visto que a fórmula da regressão linear ficou:\n",
    "CRA = 1.69701142 + 0.10377072 * Cálculo1 + 0.04829928 * LPT + 0.16400561 * P1 + 0.38324514 * IC + 0.02077901 * Cálculo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agora serão comparados os coeficientes encontrados com o do scikit learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Coefficients: ', array([[0.10304143, 0.0464367 , 0.16409834, 0.38117843, 0.02027816]]))\n",
      "('Intercept: ', array([1.73771151]))\n"
     ]
    }
   ],
   "source": [
    "# Cria objeto de regressao linear\n",
    "regr = linear_model.LinearRegression()\n",
    "\n",
    "# Treina o modelo utilizando os dados de treino\n",
    "regr.fit(X[:, 1:], Y)\n",
    "\n",
    "print('Coefficients: ', regr.coef_)\n",
    "print('Intercept: ', regr.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pode ser visto que a fórmula da regressão linear ficou:\n",
    "CRA = 1.73771151 + 0.10304143 * Cálculo1 + 0.0464367 * LPT + 0.16409834 * P1 + 0.38117843 * IC + 0.02027816 * Cálculo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apesar de não ter obtido os mesmos valores na implementação comparado com o scikit learn, os valores foram bastante similares, o que indica que o algoritmo foi implementado corretamente."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
